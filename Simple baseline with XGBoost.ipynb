{"cells":[{"metadata":{},"cell_type":"markdown","source":"This baseline has reached Top %11 with rank of #457/4540 Teams at Private Leader Board (missed Bronze with only 2 places)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sys\nimport gc\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\n\npd.options.display.precision = 15","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_set = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"segments = int(np.floor(train_set.shape[0] / 150000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(index=range(segments), dtype=np.float64)\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_generate(df,x,seg):\n    df.loc[seg, 'ave'] = x.mean()\n    df.loc[seg, 'std'] = x.std()\n    df.loc[seg, 'max'] = x.max()\n    df.loc[seg, 'min'] = x.min()\n    df.loc[seg, 'sum'] = x.sum()\n    df.loc[seg, 'mad'] = x.mad()\n    df.loc[seg, 'kurtosis'] = x.kurtosis()\n    df.loc[seg, 'skew'] = x.skew()\n    df.loc[seg, 'quant0_01'] = np.quantile(x,0.01)\n    df.loc[seg, 'quant0_05'] = np.quantile(x,0.05)\n    df.loc[seg, 'quant0_95'] = np.quantile(x,0.95)\n    df.loc[seg, 'quant0_99'] = np.quantile(x,0.99)\n    df.loc[seg, 'abs_min'] = np.abs(x).min()\n    df.loc[seg, 'abs_max'] = np.abs(x).max()\n    df.loc[seg, 'abs_mean'] = np.abs(x).mean()\n    df.loc[seg, 'abs_std'] = np.abs(x).std()\n    df.loc[seg, 'mean_change_abs'] = np.mean(np.diff(x))\n    df.loc[seg, 'max_to_min'] = x.max() / np.abs(x.min())\n    df.loc[seg, 'max_to_min_diff'] = x.max() - np.abs(x.min())\n    df.loc[seg, 'count_big'] = len(x[np.abs(x) > 500])\n    df.loc[seg, 'average_first_10000'] = x[:10000].mean()\n    df.loc[seg, 'average_last_10000']  =  x[-10000:].mean()\n    df.loc[seg, 'average_first_50000'] = x[:50000].mean()\n    df.loc[seg, 'average_last_50000'] = x[-50000:].mean()\n    df.loc[seg, 'std_first_10000'] = x[:10000].std()\n    df.loc[seg, 'std_last_10000']  =  x[-10000:].std()\n    df.loc[seg, 'std_first_50000'] = x[:50000].std()\n    df.loc[seg, 'std_last_50000'] = x[-50000:].std()\n    df.loc[seg, '10q'] = np.percentile(x, 0.10)\n    df.loc[seg, '25q'] = np.percentile(x, 0.25)\n    df.loc[seg, '50q'] = np.percentile(x, 0.50)\n    df.loc[seg, '75q'] = np.percentile(x, 0.75)\n    df.loc[seg, '90q'] = np.percentile(x, 0.90)\n    df.loc[seg, 'abs_1q'] = np.percentile(x, np.abs(0.01))\n    df.loc[seg, 'abs_5q'] = np.percentile(x, np.abs(0.05))\n    df.loc[seg, 'abs_30q'] = np.percentile(x, np.abs(0.30))\n    df.loc[seg, 'abs_60q'] = np.percentile(x, np.abs(0.60))\n    df.loc[seg, 'abs_95q'] = np.percentile(x, np.abs(0.95))\n    df.loc[seg, 'abs_99q'] = np.percentile(x, np.abs(0.99))\n    df.loc[seg, 'hilbert_mean'] = np.abs(hilbert(x)).mean()\n    df.loc[seg, 'hann_window_mean'] = (convolve(x, hann(150), mode = 'same') / sum(hann(150))).mean()    \n\n    for windows in [10, 100, 1000]:\n        x_roll_std = x.rolling(windows).std().dropna().values\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n        df.loc[seg, 'avg_roll_std' + str(windows)] = x_roll_std.mean()\n        df.loc[seg, 'std_roll_std' + str(windows)] = x_roll_std.std()\n        df.loc[seg, 'max_roll_std' + str(windows)] = x_roll_std.max()\n        df.loc[seg, 'min_roll_std' + str(windows)] = x_roll_std.min()\n        df.loc[seg, '1q_roll_std' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        df.loc[seg, '5q_roll_std' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        df.loc[seg, '95q_roll_std' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        df.loc[seg, '99q_roll_std' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        df.loc[seg, 'av_change_abs_roll_std' + str(windows)] = np.mean(np.diff(x_roll_std))\n        df.loc[seg, 'abs_max_roll_std' + str(windows)] = np.abs(x_roll_std).max()\n        df.loc[seg, 'avg_roll_mean' + str(windows)] = x_roll_mean.mean()\n        df.loc[seg, 'std_roll_mean' + str(windows)] = x_roll_mean.std()\n        df.loc[seg, 'max_roll_mean' + str(windows)] = x_roll_mean.max()\n        df.loc[seg, 'min_roll_mean' + str(windows)] = x_roll_mean.min()\n        df.loc[seg, '1q_roll_mean' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        df.loc[seg, '5q_roll_mean' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        df.loc[seg, '95q_roll_mean' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        df.loc[seg, '99q_roll_mean' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        df.loc[seg, 'av_change_abs_roll_mean' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        df.loc[seg, 'abs_max_roll_mean' + str(windows)] = np.abs(x_roll_mean).max()   \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for s in range(segments):\n    seg = train_set.iloc[s*150000:s*150000+150000]\n    x = pd.Series(seg['acoustic_data'].values)\n    y = seg['time_to_failure'].values[-1]\n    y_train.loc[s, 'time_to_failure'] = y\n    X_train = feature_generate(X_train,x,s)\ncolumns=X_train.columns  \ndel train_set\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\ny_train = y_train.values.flatten()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb\nmodel = xgb.XGBRegressor(objective = 'reg:linear',\n                         metric = 'mae',\n                         tree_method = 'gpu_hist',\n                         verbosity = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\nprint(model.feature_importances_)\npyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nplot_importance(model)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=columns, dtype=np.float64, index=submission.index)\nfor s in X_test.index:\n    seg = pd.read_csv('../input/test/' + s + '.csv')\n    x = pd.Series(seg['acoustic_data'].values)\n    X_test = feature_generate(X_test,x,s)\nX_test = scaler.transform(X_test)\nsubmission['time_to_failure'] = model.predict(X_test).clip(0, 16)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}